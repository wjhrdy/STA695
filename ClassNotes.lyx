#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble


\usepackage{amsfonts}%Typical math resource packages
\usepackage{framed}\usepackage{color}
\usepackage[english]{babel}

%\usepackage{setspace}
\newtheorem{theorem}{Theorem}[section]

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}{Lemma}[section]

\setlength{\hoffset}{-0pt}
\setlength{\textwidth}{6.0 in}
\setlength{\textheight}{8.6in}
\setlength{\oddsidemargin}{0.2 in}
\setlength{\evensidemargin}{0.2 in}
\setlength{\topmargin}{-0.3in}

\def\widenspacing{\multiply\baselineskip by 130
    \divide\baselineskip by 100}
\end_preamble
\use_default_options false
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ededed
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 2cm
\rightmargin 3cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\align center

\series bold
\size larger
Some Topics in Statistical Computation Using R 
\begin_inset Newline newline
\end_inset

 
\end_layout

\begin_layout Standard
\align center
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
\align center

\shape smallcaps
Sta 695 002 Spring 2015
\shape default
 
\begin_inset VSpace 0.2in
\end_inset

 
\size large
Department of Statistics 
\begin_inset Newline newline
\end_inset

 University of Kentucky 
\begin_inset Newline newline
\end_inset

 Lexington, KY 40536 
\begin_inset Newline newline
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
\align center

\series bold
\size small
SUMMARY
\series default
 
\end_layout

\begin_layout Standard

\size small
These notes are based on a class of Sta 695, given by Dr.
 Mai Zhou in Spring 2015.
 The actual materials covered in class may be different.
 
\end_layout

\begin_layout Standard

\shape slanted
AMS 2000 Subject Classification:
\shape default
 Primary 60E15; secondary 60G30.
\end_layout

\begin_layout Standard

\shape slanted
Key Words and Phrases: Truncated data, NPMLE, square error loss.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Make sure you run
\end_layout

\begin_layout Plain Layout
install.packages("Rmpfr")
\end_layout

\begin_layout Plain Layout
install.packages("knitr")
\end_layout

\begin_layout Plain Layout
install.packages("penalized")
\end_layout

\begin_layout Plain Layout
install.packages(
\begin_inset Quotes eld
\end_inset

RCurl
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout
install.packages(
\begin_inset Quotes eld
\end_inset

ELYP
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout
install.packages(
\begin_inset Quotes eld
\end_inset

survival
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Plain Layout
in r before compiling
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
We aim to cover some topics that are 
\shape italic
interesting
\shape default
 and 
\shape italic
useful
\shape default
 and related to R.
\begin_inset Newline newline
\end_inset

We avoid topics that are too theoretical.
 We try to make the level of the class in between Sta 605 and Sta 705.
 Finally, two semesters of Statistical Inference (based on Casella and Berger,
 for example) is assumed.
\end_layout

\begin_layout Section
Errors in the R Numerical Computation
\end_layout

\begin_layout Enumerate
Errors cause by decimal numbers being represented as binary numbers
\begin_inset Newline newline
\end_inset

 Question 1:
\begin_inset Newline newline
\end_inset

 Why R tells us 
\begin_inset Formula $0.3-0.2\neq0.1$
\end_inset

?
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

0.3 - 0.2 == 0.1 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

 Question 2:
\begin_inset Newline newline
\end_inset

 If you run the following code in R, it will never stop.
 Why?
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

V = 1
\end_layout

\begin_layout Plain Layout

while(V != 0){
\end_layout

\begin_layout Plain Layout

V = V - 0.1
\end_layout

\begin_layout Plain Layout

print(V)}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

The results will be endless.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Answer:
\begin_inset Newline newline
\end_inset

This is because decimal numbers are expressed in the following form in R:
\begin_inset Newline newline
\end_inset

 
\begin_inset Formula 
\begin{eqnarray}
\sum_{k=1}^{64}\dfrac{\delta_{k}}{2^{k}} &  & \textrm{where}\,\delta=0\,\textrm{or}\,1
\end{eqnarray}

\end_inset

So, when programming in R, try to avoid the use of 
\begin_inset Quotes eld
\end_inset

equal to
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

not equal to.
\begin_inset Quotes erd
\end_inset

 The following code is a better practice:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

V = 1
\end_layout

\begin_layout Plain Layout

while(V > 1e-9){
\end_layout

\begin_layout Plain Layout

V = V - 0.1
\end_layout

\begin_layout Plain Layout

print(V)} 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Use log as a parameter in the function to make the result more accurate.
 
\begin_inset Newline newline
\end_inset

Sometimes, there are options built in to common R functions which allow
 you to get more accurate results.
 Here's an example using the dnorm function:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

log(dnorm(80,0,2))
\end_layout

\begin_layout Plain Layout

dnorm(80,0,2,log=T)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Discrepancies found when calculating both sides of the following equality
 in R when 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are large:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{equation}
y\times\left(\sqrt{x+1}-\sqrt{x}\right)=\dfrac{y}{\sqrt{x+1}+\sqrt{x}}
\end{equation}

\end_inset

Question:
\begin_inset Newline newline
\end_inset

When the values of 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are very large, which of the above expressions is more accurate?
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<warning=FALSE,message=FALSE>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

x = 1e12
\end_layout

\begin_layout Plain Layout

y=666666666666
\end_layout

\begin_layout Plain Layout

y*(sqrt(x+1)-sqrt(x))
\end_layout

\begin_layout Plain Layout

y/(sqrt(x+1)+sqrt(x)) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

We can find a more accurate solution by requiring R to make more accurate
 calculations using the rmpfr package:\SpecialChar \textcompwordmark{}

\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<warning=FALSE,message=FALSE>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

require(Rmpfr)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

x = 1e12
\end_layout

\begin_layout Plain Layout

y=666666666666
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

myx = mpfr(x,128) 
\end_layout

\begin_layout Plain Layout

myy = mpfr(y,128) 
\end_layout

\begin_layout Plain Layout

myy*(sqrt(myx+1)-sqrt(myx)) 
\end_layout

\begin_layout Plain Layout

myy/(sqrt(myx+1)+sqrt(myx))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

The above code shows that the expression on the right-hand side of equation
 (2) is better.
 Then what makes the other expression not as good?
\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset

 Answer:
\begin_inset Newline newline
\end_inset

When you compute the square root of a number in R, there will be small errors
 in the decimal values.
 So, when you compute 
\begin_inset Formula $\sqrt{x+1}-\sqrt{x}$
\end_inset

, the digits in front which are more accurate are canceled out and you're
 left with the inaccurate digits.
 So, your result won't be the true difference between the two square roots,
 but the difference in their erroneous decimal approximations.
 Thus, the right-hand side of equation (2) is better because the denominator
 involving the square roots is less influenced by those small calculation
 errors.
\end_layout

\begin_layout Section
Confidence Intervals Based on the LRT
\end_layout

\begin_layout Enumerate
Let's consider the binomial case
\begin_inset Newline newline
\end_inset

Let 
\begin_inset Formula $X$
\end_inset

 be a random variable distributed as a Binomial
\begin_inset Formula $\left(n,p\right)$
\end_inset

.
 Then, we know that the maximum likelihood estimator of p will be 
\begin_inset Formula $\hat{p}=\frac{x}{n}$
\end_inset

.
 And, from our introductory statistics courses, we know that we can calculate
 a confidence interval for 
\begin_inset Formula $p$
\end_inset

 by the following formula for a Wald confidence interval: 
\begin_inset Formula 
\begin{equation}
\hat{p}\pm z\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}
\end{equation}

\end_inset

One issue with this type of confidence interval is that it's possible for
 it to contain values outside of the interval 
\begin_inset Formula $(0,1)$
\end_inset

, which doesn't make any sense.
 There are some known transformations which eliminate (or reduce the likelihood)
 of this possibility.
 So, let's consider an arbitrary transformation 
\begin_inset Formula $g\left(p\right)$
\end_inset

 and construct a confidence interval for it.
 From the formula for a Wald confidence interval, we have: 
\begin_inset Formula 
\begin{equation}
g\left(\hat{p}\right)\pm z\sqrt{Var\left(g\left(\hat{p}\right)\right)}
\end{equation}

\end_inset

And since the MLE is invariant, we know that 
\begin_inset Formula $\widehat{g\left(p\right)}=g\left(\hat{p}\right)$
\end_inset

 So, by the Delta method, we have: 
\begin_inset Formula 
\begin{equation}
Var\left(g\left(\hat{p}\right)\right)\approx\left[g'(p)\right]^{2}Var\left(\hat{p}\right)
\end{equation}

\end_inset

Which gives us the following confidence interval 
\begin_inset Formula $g(p)$
\end_inset

: 
\begin_inset Formula $g(p)\in[L,U]$
\end_inset

.
 Which yields the following confidence interval for 
\begin_inset Formula $p$
\end_inset

: 
\begin_inset Formula $p\in[g^{-1}(L),g^{-1}(U)]$
\end_inset

 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

So, why do we use transformations and why are they useful? 
\begin_inset Newline newline
\end_inset

If you use a 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 transformation, then you can guarantee that the confidence interval for
 
\begin_inset Formula $p$
\end_inset

 will be between 
\begin_inset Formula $(0,1)$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

But, there exist many different 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 transformations.
 Which one is the best?(kind of arbitrary, accurate, speed, etc.).
\begin_inset Newline newline
\end_inset

The selection of the best 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 transformation is somewhat arbitrary.
 Your decision could be based on accuracy, speed, or any other criteria
 that you deem important.
 An example of this arbitrariness is the discrepancy between the transformation
 used by SAS and that used by R.
 In SAS, they use 
\begin_inset Formula $log(-log(1-p))$
\end_inset

, while R uses 
\begin_inset Formula $log(1-p)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Hypothesis Testing
\begin_inset Newline newline
\end_inset

Let's consider testing the following hypotheses:
\begin_inset Newline newline
\end_inset

 
\begin_inset Formula 
\begin{align}
H_{0}: & p=p_{0}\\
H_{1}: & p\neq p_{0}
\end{align}

\end_inset

Using the likelihood ratio test, we would reject the null when: 
\begin_inset Formula 
\begin{align}
log\left[\frac{likelihood(x,p_{0})}{likelihood(x,\hat{p})}\right] & >C=\chi_{1,0.95}^{2}
\end{align}

\end_inset

We can simplify the left-hand side of this inequality to:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{equation}
xlog(p)+(n-x)log(1-p)-xlog(\hat{p})-(n-x)log(1-\hat{p})=xlog(\frac{p}{\hat{p}})+(n-x)log(\frac{1-p}{1-\hat{p}})
\end{equation}

\end_inset

Now, there are two conditions the we need to take into consideration:
\begin_inset Newline newline
\end_inset

 a.
 p
\begin_inset Formula $\neq$
\end_inset

0 and p
\begin_inset Formula $\neq$
\end_inset

1
\begin_inset Newline newline
\end_inset

 b.
 
\begin_inset Formula $\hat{p}$
\end_inset


\begin_inset Formula $\neq$
\end_inset

0 and 
\begin_inset Formula $\hat{p}$
\end_inset


\begin_inset Formula $\neq$
\end_inset

1
\begin_inset Newline newline
\end_inset

We can write a function in R which calculates this value for us:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

BinLLR = function(n,x,p){
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

if(x >= n) stop("100 percent success?")
\end_layout

\begin_layout Plain Layout

if(p <= 0) stop("P must between 0 and 1") 
\end_layout

\begin_layout Plain Layout

if(x <= 0) stop 
\end_layout

\begin_layout Plain Layout

if(p >= 1) stop 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

phat = x/n 
\end_layout

\begin_layout Plain Layout

-2*(x*log(p/phat)+(n-x)*log(1-p)/(1-phat))}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

ts = BinLLR(n=100,x=30,p=0.5) 
\end_layout

\begin_layout Plain Layout

1 - pchisq(ts,df=1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Good Properties of the Likelihood Ratio Test Approach to Confidence Intervals
\end_layout

\begin_deeper
\begin_layout Enumerate
The LRT method is transformation invariant
\end_layout

\begin_deeper
\begin_layout Enumerate
Monotone Transformations
\begin_inset Newline newline
\end_inset

If you have a confidence interval for 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $(lower,\,upper)$
\end_inset

, and you want a confidence interval for 
\begin_inset Formula $log(p)$
\end_inset

 (or any monotone function), you can find it by taking the log of the lower
 and upper bounds for 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $\left(log(lower),\,log(upper)\right)$
\end_inset

.
 This is much easier than the Wald method which involves calculating derivatives
, using the delta method, etc.
\end_layout

\begin_layout Enumerate
Transformations Which Aren't Monotone
\begin_inset Newline newline
\end_inset

This property even holds for transformations which aren't monotone.
 If you have a transformation 
\begin_inset Formula $g(p)$
\end_inset

 which isn't monotone and a confidence interval for 
\begin_inset Formula $p$
\end_inset

, 
\begin_inset Formula $(l,\,u)$
\end_inset

, then you can find your transformed confidence interval by the following
 steps:
\begin_inset Formula 
\begin{align*}
upper_{new} & =\max_{l<p<u}g(p)\\
lower_{new} & =\min_{l<p<u}g(p)
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
The LRT method is range preserving
\begin_inset Newline newline
\end_inset

This means that the confidence intervals it produces will always be inside
 of the parameter space.
 So, in the binomial case, the confidence intervals produced by the LRT
 method will always be between 
\begin_inset Formula $(0,1)$
\end_inset

.
\end_layout

\begin_layout Enumerate
The LRT method doesn't necessarily give symmetric intervals
\begin_inset Newline newline
\end_inset

The asymmetry of the intervals is more similar to the actual distributions
 under consideration.
 So, in the binomial case when we have a small value of 
\begin_inset Formula $p$
\end_inset

, we know that there are more values above 
\begin_inset Formula $p$
\end_inset

 than below it, so we'd expect our interval to reflect this.
 The Wald interval, on the other hand, will always be symmetric which we
 know is not always true of the distributions under consideration.
\end_layout

\end_deeper
\begin_layout Enumerate
Possible Draw Backs of the Likelihood Ratio Test Approach to Confidence
 Intervals
\end_layout

\begin_deeper
\begin_layout Enumerate
You may not have to use the delta method or take any derivatives, but you
 still have to find maximums and roots.
 These calculations are easily done using computers, so this isn't too much
 of an issue.
\end_layout

\begin_layout Enumerate
This method may not produce the narrowest intervals in all cases.
 But, you can get narrower intervals by fine-tuning your critical value
 of 3.84.
\end_layout

\end_deeper
\begin_layout Section
The Ratio of Two Success Probabilities
\end_layout

\begin_layout Standard
Let's consider the case where we have two samples, each containing a success
 probability.
 From each sample we will have an 
\begin_inset Formula $n_{i}$
\end_inset

 and an 
\begin_inset Formula $x_{i}$
\end_inset

, where 
\begin_inset Formula $n_{i}$
\end_inset

 is the total number of units observed in the 
\begin_inset Formula $i^{th}$
\end_inset

 sample and 
\begin_inset Formula $x_{i}$
\end_inset

 is the total number of successes observed in the 
\begin_inset Formula $i^{th}$
\end_inset

 sample.
 We can model this situation by two binomial random variables, 
\begin_inset Formula $X_{1}$
\end_inset

 and 
\begin_inset Formula $X_{2}$
\end_inset

, where 
\begin_inset Formula $X_{1}\sim Binomial\left(n_{1},p_{1}\right)$
\end_inset

 and 
\begin_inset Formula $X_{2}\sim Binomial\left(n_{2},p_{2}\right)$
\end_inset

, where the MLE for 
\begin_inset Formula $p_{1}$
\end_inset

 is 
\begin_inset Formula $\widehat{p_{1}}=\frac{x_{1}}{n_{1}}$
\end_inset

 and the MLE for 
\begin_inset Formula $p_{2}$
\end_inset

 is 
\begin_inset Formula $\widehat{p_{2}}=\frac{x_{2}}{n_{2}}$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Now, let's suppose that we want to use the likelihood ratio test method
 of creating a confidence interval for the ratio of 
\begin_inset Formula $\frac{p_{1}}{p_{2}}$
\end_inset

.
 Since maximum likelihood estimators are invariant, we know that the MLE
 for 
\begin_inset Formula $\frac{p_{1}}{p_{2}}$
\end_inset

 will simply be 
\begin_inset Formula $\frac{\hat{p_{1}}}{\hat{p_{2}}}=\frac{x_{1}n_{2}}{x_{2}n_{1}}$
\end_inset

.
 We also know that since the log-likelihood ratio is additive, it follows
 that 
\begin_inset Formula $LLR=LLR\left(\textrm{sample_{1}}\right)+LLR\left(\textrm{sample_{2}}\right)$
\end_inset

.
 And since we want to use the likelihood ratio test method, we'll need to
 consider the following hypotheses:
\begin_inset Formula 
\begin{align*}
H_{0}: & \frac{p_{1}}{p_{2}}=\theta\\
H_{1}: & \frac{p_{1}}{p_{2}}\neq\theta
\end{align*}

\end_inset

Under the likelihood ratio test, our initial parameters of interest are
 
\begin_inset Formula $p_{1}$
\end_inset

 and 
\begin_inset Formula $p_{2}$
\end_inset

.
 But this is equivalent to considering the parameters 
\begin_inset Formula $p_{1}$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

, since 
\begin_inset Formula $\theta=\frac{p_{1}}{p_{2}}$
\end_inset

.
 If we take 
\begin_inset Formula $p_{1}$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 to be our parameters of interest, then it follows that 
\begin_inset Formula $p_{1}$
\end_inset

 can be viewed as our nuisance parameter since 
\begin_inset Formula $\theta$
\end_inset

 is what we're truly interested in.
 From our previous coursework, we know that we can get rid of a nuisance
 parameter in the log-likelihood ratio by simply 
\begin_inset Quotes eld
\end_inset

maximizing it out.
\begin_inset Quotes erd
\end_inset

 In this case, we'll have:
\begin_inset Formula 
\begin{align*}
LLR\left(\theta\right) & =\max_{p_{1}}\left\{ LLR\left(\textrm{sample_{1}}\right)+LLR\left(\textrm{sample_{2}}\right)\right\} \\
 & =\max_{\left\{ p_{1},p_{2}:\frac{p_{1}}{p_{2}}=\theta\right\} }\left\{ LLR\left(\textrm{sample_{1}}\right)+LLR\left(\textrm{sample_{2}}\right)\right\} 
\end{align*}

\end_inset

And then we can multiply this by 
\begin_inset Formula $-2$
\end_inset

 to obtain our likelihood ratio test statistic:
\begin_inset Formula 
\begin{align*}
-2\left[LLR\left(\theta\right)\right] & =-2\left[\max_{\left\{ p_{1},p_{2}:\frac{p_{1}}{p_{2}}=\theta\right\} }\left\{ LLR\left(\textrm{sample_{1}}\right)+LLR\left(\textrm{sample_{2}}\right)\right\} \right]\\
 & =\min_{\left\{ p_{1},p_{2}:\frac{p_{1}}{p_{2}}=\theta\right\} }\left\{ -2\cdot LLR\left(\textrm{sample_{1}}\right)-2\cdot LLR\left(\textrm{sample_{2}}\right)\right\} 
\end{align*}

\end_inset

So, once we have our likelihood ratio test statistic, we can compare it
 to 
\begin_inset Formula $3.84$
\end_inset

 and determine whether or not we reject 
\begin_inset Formula $H_{0}$
\end_inset

.
 Then, we can create a confidence interval for 
\begin_inset Formula $\theta=\frac{p_{1}}{p_{2}}$
\end_inset

 by finding the values of 
\begin_inset Formula $\theta$
\end_inset

 which give us a likelihood ratio test statistic less than 
\begin_inset Formula $3.84$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Below is Dr.
 Zhou's R code for the computing a confidence interval for the ratio of
 two success probabilities using the likelihood ratio test method.
 Note that the code is for 
\begin_inset Formula $\theta=\frac{p_{2}}{p_{1}}$
\end_inset

 and not 
\begin_inset Formula $\theta=\frac{p_{1}}{p_{2}}$
\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<eval=FALSE, tidy=FALSE>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

BinRatio <- function(n1, x1, n2, x2, step=0.1, initStep=0.1, level=3.84){
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

if(x1 >= n1) stop("all success?")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

MLE <- (x2*n1)/(x1*n2)   #### recall we are to find CI for p2/p1, not p1/p2.
\end_layout

\begin_layout Plain Layout

EPS <- .Machine$double.eps
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Theta <- function(theta, n1=n1, x1=x1, n2=n2, x2=x2)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

       llr <- function(const, n1, x1, n2, x2, theta) {
\end_layout

\begin_layout Plain Layout

              npllik1 <- -2*(x1*log((const*n1)/x1) + 
\end_layout

\begin_layout Plain Layout

                           (n1-x1)*log(((1-const)*n1)/(n1-x1)))
\end_layout

\begin_layout Plain Layout

              npllik2 <- -2*(x2*log((const*theta*n2)/x2) + 
\end_layout

\begin_layout Plain Layout

                           (n2-x2)*log(((1-const*theta)*n2)/(n2-x2)))
\end_layout

\begin_layout Plain Layout

        return(npllik1 + npllik2)
\end_layout

\begin_layout Plain Layout

        }
\end_layout

\begin_layout Plain Layout

       upBD <- min( 1-EPS, 1/theta - EPS)
\end_layout

\begin_layout Plain Layout

       temp <- optimize(f = llr, 
\end_layout

\begin_layout Plain Layout

                        lower = EPS, 
\end_layout

\begin_layout Plain Layout

                        upper = upBD, 
\end_layout

\begin_layout Plain Layout

                        n1 = n1, 
\end_layout

\begin_layout Plain Layout

                        x1 = x1, 
\end_layout

\begin_layout Plain Layout

                        n2 = n2, 
\end_layout

\begin_layout Plain Layout

                        x2 = x2,
\end_layout

\begin_layout Plain Layout

                        theta = theta)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    cstar <- temp$minimum
\end_layout

\begin_layout Plain Layout

    val <- temp$objective
\end_layout

\begin_layout Plain Layout

    pvalue <- 1 - pchisq( val, df=1)
\end_layout

\begin_layout Plain Layout

    list(`-2LLR` = val, cstar = cstar, Pval=pvalue)
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

value <- 0
\end_layout

\begin_layout Plain Layout

step1 <- step
\end_layout

\begin_layout Plain Layout

Lbeta <- MLE - initStep
\end_layout

\begin_layout Plain Layout

for( i in 1:8 ) {
\end_layout

\begin_layout Plain Layout

while(value < level) {
\end_layout

\begin_layout Plain Layout

Lbeta <- Lbeta - step1
\end_layout

\begin_layout Plain Layout

value <- Theta(theta = Lbeta, n1=n1, x1=x1, n2=n2, x2=x2)$'-2LLR'
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

Lbeta <- Lbeta + step1
\end_layout

\begin_layout Plain Layout

step1 <- step1/10
\end_layout

\begin_layout Plain Layout

value <- Theta( theta =Lbeta, n1=n1, x1=x1, n2=n2, x2=x2)$'-2LLR'
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

value1 <- value
\end_layout

\begin_layout Plain Layout

value <- 0
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Ubeta <- MLE + initStep
\end_layout

\begin_layout Plain Layout

for( i in 1:8 ) {
\end_layout

\begin_layout Plain Layout

   while(value < level) {
\end_layout

\begin_layout Plain Layout

    Ubeta <- Ubeta + step
\end_layout

\begin_layout Plain Layout

    value <- Theta(theta=Ubeta, n1=n1, x1=x1, n2=n2, x2=x2 )$'-2LLR'
\end_layout

\begin_layout Plain Layout

   }
\end_layout

\begin_layout Plain Layout

   Ubeta <- Ubeta - step
\end_layout

\begin_layout Plain Layout

   step <- step/10
\end_layout

\begin_layout Plain Layout

   value <- Theta(theta=Ubeta, n1=n1, x1=x1, n2=n2, x2=x2)$'-2LLR'
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

return( list(Low=Lbeta, Up=Ubeta, FstepL=step1, FstepU=step,
\end_layout

\begin_layout Plain Layout

Lvalue=value1, Uvalue=value) )
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
The Weibull Distribution
\end_layout

\begin_layout Standard
\noindent
We know from survival analysis that a random variable, 
\begin_inset Formula $X$
\end_inset

, following a Weibull distribution will satisfy the following probability:
\begin_inset Formula 
\[
P\left(X>t\right)=e^{-\left(\lambda t\right)^{\beta}}\,\,\,\textrm{for}\,t\geq0\,\textrm{and}\,\lambda,\beta>0
\]

\end_inset

If we have a sample of size 
\begin_inset Formula $n$
\end_inset

 from a Weibull distribution, then we can derive the log-likelihood function
 for 
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

:
\begin_inset Formula 
\[
log\left(L\left(\lambda,\beta|X_{1},\ldots,X_{n}\right)\right)=\sum_{i=1}^{n}\left[log\left(\beta\right)+\left(\beta-1\right)log\left(\lambda x_{i}\right)+log\left(\lambda\right)-\left(\lambda x_{i}\right)^{\beta}\right]
\]

\end_inset

But in survival analysis, it is common to have censored data.
 This means that we have some 
\begin_inset Formula $X$
\end_inset

 values that we don't know the exact value of, only that they're greater
 than some specific value.
 As an example, consider the case where you track ants for a 12 month period
 and you record the time at which they die.
 If, at the end of that 12 month period, an ant is still alive, then all
 you know is that it survived for longer than 12 months.
 So, if 
\begin_inset Formula $\textrm{ant_{9}}=X_{9}$
\end_inset

 is still alive, then you know that it survived for some time greater than
 
\begin_inset Formula $12$
\end_inset

 months.
 So, the survival value of 
\begin_inset Formula $\textrm{ant_{9}}$
\end_inset

 is 
\begin_inset Formula $X_{9}>12$
\end_inset

, which we denote by 
\begin_inset Formula $X_{9}=12^{+}$
\end_inset

.
 When dealing with survival data, it's common to represent it with two vectors.
 The first vector contains the survival values, and the second vector is
 the corresponding 
\begin_inset Quotes eld
\end_inset

status
\begin_inset Quotes erd
\end_inset

 vector, which contains a 
\begin_inset Formula $1$
\end_inset

 if the corresponding survival value is a true value and a 
\begin_inset Formula $0$
\end_inset

 if the corresponding survival value is a censored value.
 Using the survival package in R, you can convert a a pair of vectors into
 survival data using the Surv function.
 Here's an example:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<warning=FALSE,message=FALSE,error=FALSE>>=
\end_layout

\begin_layout Plain Layout

require(survival)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

x = c(5,3,7,2,8,9,5,4,6,2)
\end_layout

\begin_layout Plain Layout

d = c(0,1,1,1,1,1,0,0,1,1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Surv(x,d)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

How does censored data change our likelihood function?
\begin_inset Newline newline
\end_inset

For an observation with a status of 
\begin_inset Formula $1$
\end_inset

, its contribution to the likelihood function is simply 
\begin_inset Formula $f\left(x_{i}|\lambda,\beta\right)$
\end_inset

.
 For an observation with a status of 
\begin_inset Formula $0$
\end_inset

, its contribution to the likelihood function is the probability 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $P\left(X>x_{i}\right)=e^{-\left(\lambda x_{i}\right)^{\beta}}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
.
 So, if we have some censored data, then our likelihood function becomes:
\begin_inset Formula 
\[
L\left(\lambda,\beta|X_{1},\ldots,X_{n}\right)=\prod_{i=1}^{n}\left[f\left(x_{i}|\lambda,\beta\right)\right]^{\delta_{i}}\left[e^{-\left(\lambda x_{i}\right)^{\beta}}\right]^{1-\delta_{i}}
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
where 
\begin_inset Formula $\delta_{i}$
\end_inset

 is the status of the 
\begin_inset Formula $i^{th}$
\end_inset

 observation
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
.
 Below is Dr.
 Zhou's R code for the computing the log-likelihood for a Weibull distribution
 with censored data:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<eval=FALSE>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

WeibLL <- function(x, d, lam, beta){
\end_layout

\begin_layout Plain Layout

  ###  lam  can be a vector of length(x), but use with care.
\end_layout

\begin_layout Plain Layout

  ###  beta must be a scalar.
\end_layout

\begin_layout Plain Layout

  ##############################
\end_layout

\begin_layout Plain Layout

  ### if you use survreg of Survival package, then beta=1/scale
\end_layout

\begin_layout Plain Layout

  ### and  lam=exp(-location)
\end_layout

\begin_layout Plain Layout

  ############################################
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  if (any((d != 0) & (d != 1) ))
\end_layout

\begin_layout Plain Layout

    stop("d must be 0(right-censored) or 1(uncensored)")
\end_layout

\begin_layout Plain Layout

  if(any(lam <= 0)) stop("lam must >0")
\end_layout

\begin_layout Plain Layout

  if(beta <= 0) stop("beta must > 0")
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  sum(d*(log(beta)+(beta-1)*log(x)+ beta*log(lam))) - sum((x*lam)^beta)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Finding the MLE
\begin_inset Newline newline
\end_inset

Calculating the MLE of the Weibull distribution can be difficult, especially
 with censored data, so we'll use a pre-made function in R.
 Under the survival package, we can use the survreg function to calculate
 the MLE by just fitting the intercept.
 Here's some sample code:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

t = c(5,4,6,8,11,12,16,14,21,26,33,44)
\end_layout

\begin_layout Plain Layout

d = c(1,0,1,1,1,1,1,1,1,0,1,1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

survreg( Surv(t,d) ~ 1, dist = "weibull")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

One issue with this function is that it doesn't return the MLE's for the
 Weibull distribution, but the MLE's for the extreme value distribution
 which is just the log of a Weibull distribution.
 Since the MLE is invariant, we know that we can convert the MLE of the
 extreme value distribution to that of the Weibull distribution by a transformat
ion.
 By using the following transformations, we can find the MLE for the Weibull
 distribution:
\begin_inset Formula 
\begin{align*}
\lambda & =e^{-location}\\
\beta & =\frac{1}{scale}
\end{align*}

\end_inset

where 
\begin_inset Formula $location$
\end_inset

 and 
\begin_inset Formula $scale$
\end_inset

 are the parameters of the extreme value distribution.
 In R, we can find the MLE of a Weibull distribution in the following way:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

t = c(5,4,6,8,11,12,16,14,21,26,33,44)
\end_layout

\begin_layout Plain Layout

d = c(1,0,1,1,1,1,1,1,1,0,1,1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

reg = survreg( Surv(t,d) ~ 1, dist = "weibull")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

location = as.vector(reg$coefficients)
\end_layout

\begin_layout Plain Layout

scale = as.vector(reg$scale)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

exp(-location)
\end_layout

\begin_layout Plain Layout

1/scale
\end_layout

\begin_layout Plain Layout

reg$loglik
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Maxi <- reg$loglik[1]
\end_layout

\begin_layout Plain Layout

print(Maxi)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Now that we have the MLE, the next step is to turn this into a confidence
 interval for the survival probability, 
\begin_inset Formula $P\left(X>t\right)=e^{-\left(\lambda t\right)^{\beta}}$
\end_inset

 where 
\begin_inset Formula $t$
\end_inset

 is some time point.
 So, let's first look at a plot of the likelihood function with respect
 to 
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 when we take 
\begin_inset Formula $t=15$
\end_inset

:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<tidy=FALSE>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

WeibLL <- function(x, d, lam, beta){
\end_layout

\begin_layout Plain Layout

  if (any((d != 0) & (d != 1) ))
\end_layout

\begin_layout Plain Layout

    stop("d must be 0(right-censored) or 1(uncensored)")
\end_layout

\begin_layout Plain Layout

  if(any(lam <= 0)) stop("lam must >0")
\end_layout

\begin_layout Plain Layout

  if(beta <= 0) stop("beta must > 0")
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  sum(d*(log(beta)+(beta-1)*log(x)+ beta*log(lam))) - sum((x*lam)^beta)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

lamvec <- 1:100/1000
\end_layout

\begin_layout Plain Layout

betavec <- 0.5 + 1:130/50
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

ymat <- xmat <- matrix(NA, 100, 130)
\end_layout

\begin_layout Plain Layout

for(i in 1:100) for(j in 1:130) xmat[i,j] <- WeibLL(x=t, 
\end_layout

\begin_layout Plain Layout

	                                                 d=d,
\end_layout

\begin_layout Plain Layout

                                                     lam=lamvec[i], 
\end_layout

\begin_layout Plain Layout

                                                     beta=betavec[j])
\end_layout

\begin_layout Plain Layout

for(i in 1:100) for(j in 1:130) {
\end_layout

\begin_layout Plain Layout

ymat[i,j] <- exp(-(lamvec[i]*15)^betavec[j] )
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

contour(lamvec, betavec, xmat)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

We can see that the log-likelihood function is its highest around 
\begin_inset Formula $\left(0.05,1.5\right)$
\end_inset

, which agrees with the MLE values that we found above.
 Now, we want to find a 
\begin_inset Formula $95\%$
\end_inset

 confidence interval for the survival probability given a value of 
\begin_inset Formula $t$
\end_inset

:
\begin_inset Formula 
\[
P\left(X>t\right)=g\left(\lambda,\beta\right)=e^{-\left(\lambda t\right)^{\beta}}
\]

\end_inset

So, we need to find the region of our likelihood function which is within
 
\begin_inset Formula $3.84$
\end_inset

 of our MLE.
 Let's define this region to be 
\begin_inset Formula $U$
\end_inset

.
 Once we have the region 
\begin_inset Formula $U$
\end_inset

, we can find the upper and lower bounds of our confidence interval in the
 following way:
\begin_inset Formula 
\begin{align*}
\textrm{upper} & =\max_{\lambda,\beta\in U}\left\{ exp\{-\left(\lambda t\right)\right\} \\
\textrm{lower} & =\min_{\lambda,\beta\in U}\left\{ exp\{-\left(\lambda t\right)\right\} 
\end{align*}

\end_inset

We can use R to find the region 
\begin_inset Formula $U$
\end_inset

 by using the following code:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<tidy=FALSE>>=
\end_layout

\begin_layout Plain Layout

Pfun
\end_layout

\begin_layout Plain Layout

lamvec <- 1:100/1000
\end_layout

\begin_layout Plain Layout

betavec <- 0.5 + 1:130/50
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

ymat <- xmat <- matrix(NA, 100, 130)
\end_layout

\begin_layout Plain Layout

for(i in 1:100) for(j in 1:130) xmat[i,j] <- WeibLL(x=t,
\end_layout

\begin_layout Plain Layout

                                                     d=d, 
\end_layout

\begin_layout Plain Layout

                                                     lam=lamvec[i], 
\end_layout

\begin_layout Plain Layout

                                                     beta=betavec[j])
\end_layout

\begin_layout Plain Layout

for(i in 1:100) for(j in 1:130) ymat[i,j] <- exp(-(lamvec[i]*15)^betavec[j]
 )
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

contour(lamvec, betavec, xmat, level=c( Maxi - 3.84/2))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Since we don't have an explicit formula or function for this region, we
 will have to find the confidence interval for it algorithmically.
 So, we'll have to search the region 
\begin_inset Formula $U$
\end_inset

 for a maximum and minimum value.
 An example of this process is shown below:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

contour(lamvec, betavec, xmat, level=c( Maxi - 3.84/2))
\end_layout

\begin_layout Plain Layout

par(new=TRUE)
\end_layout

\begin_layout Plain Layout

contour(lamvec, betavec, ymat, level=c(0.323, 0.24, 0.22))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

We can see that the smallest probability value which falls in our region
 
\begin_inset Formula $U$
\end_inset

 is somewhere around 
\begin_inset Formula $0.24$
\end_inset

.
 We could do the same for the upper bound, but it would also by equally
 imprecise.
 Instead, we should create a more general search method for finding maximum
 and minimum values which would work in higher dimensions as well.
 Dr.
 Zhou suggests we use the 
\begin_inset Quotes eld
\end_inset

Blind Man Climbing a Mounting
\begin_inset Quotes erd
\end_inset

 approach.
 The steps for this algorithm are:
\end_layout

\begin_layout Enumerate
\noindent
Choose a radius value.
\end_layout

\begin_layout Enumerate
Starting with the MLE, calculate the likelihood values at every (or a select
 number) point on that radius.
\end_layout

\begin_layout Enumerate
Find the largest value and take that point as your new center.
\end_layout

\begin_layout Enumerate
From your new center point, calculate the likelihood values at every (or
 a select number) point on that radius.
\end_layout

\begin_layout Enumerate
Continue this process until you have a point which lies outside your region
 
\begin_inset Formula $U$
\end_inset

.
\end_layout

\begin_layout Enumerate
Once you find one point outside of 
\begin_inset Formula $U$
\end_inset

, return to your previous point and reduce your radius value and recalculate
 you likelihood values.
\end_layout

\begin_layout Enumerate
Repeat until the radius value gets sufficiently small.
\end_layout

\begin_layout Standard
Likelihood Ratio Test Method vs.
 the Wald Method in the Weibull Case:
\begin_inset Newline newline
\end_inset

Both require work to get a confidence interval, but the Wald requires lots
 of calculations by hand while the LRT method only requires computation
 time once your search algorithm is written.
 So, we obviously prefer the LRT method.
\end_layout

\begin_layout Standard
Here is the Log likelihood ratio function that we will use for optimization
 in this case we have the maximum for this problem set as a magic number
 .
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

WLLRfn <- function(para, dataMat, Maxi=-38.53396) {
\end_layout

\begin_layout Plain Layout

  mytemp <- 2*( Maxi-WeibLL(x=dataMat[1,], d=dataMat[2,], lam=para[1], beta=para
[2]))
\end_layout

\begin_layout Plain Layout

  list("-2LLR"=mytemp)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
This is a function created by Dr.
 Zhou that will uses the steps above to find the lower value of the confidence
 interval.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

FindL2 <- function(mle, LLRfn, Pfun, dataMat, level=3.84) {
\end_layout

\begin_layout Plain Layout

  if (length(mle) != 2) stop("MLE must be of length 2")
\end_layout

\begin_layout Plain Layout

  ####
\end_layout

\begin_layout Plain Layout

  #### Pfun must be able to take vector inputs (as well as scalar)in both
\end_layout

\begin_layout Plain Layout

  #### of its arguments (b1 and b2)
\end_layout

\begin_layout Plain Layout

  #### No need to compute Max, sinve -2LLR at mle in 0.
\end_layout

\begin_layout Plain Layout

  ####
\end_layout

\begin_layout Plain Layout

  PfunLoid <- Pfun(b1=mle[1],b2=mle[2])
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  Pfun2<- function(x) Pfun(b1=x[1], b2=x[2])
\end_layout

\begin_layout Plain Layout

  stepsize <- (0.03*(1/grad(Pfun2, mle))) ####how to get 0.03?
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  b1vec <- mle[1] + stepsize[1] * (1:6 - 3.5)
\end_layout

\begin_layout Plain Layout

  b2vec <- mle[1] + stepsize[1] * (1:6 - 3.5)
\end_layout

\begin_layout Plain Layout

  temp <- matrix(NA, nrow =3, ncol=36)
\end_layout

\begin_layout Plain Layout

  temp[1, ] <- rep(b1vec, each=6)
\end_layout

\begin_layout Plain Layout

  temp[2, ] <- rep(b1vec)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  for (i in 1:6) for (j in 1:6 ) {
\end_layout

\begin_layout Plain Layout

    ind <- (i - 1) * 6 + j
\end_layout

\begin_layout Plain Layout

    mtemp <- LLRfn(para=c(b1vec[i], b2vec[j]), dataMat)
\end_layout

\begin_layout Plain Layout

    temp[3, ind] <- mtemp$"-2LLR"
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  target <- level
\end_layout

\begin_layout Plain Layout

  subsetInd <- (temp[3, ] <- target)
\end_layout

\begin_layout Plain Layout

  if ( !any(subsetInd) )
\end_layout

\begin_layout Plain Layout

    #### stepsize <- stepsize/10 ####
\end_layout

\begin_layout Plain Layout

    stop("stepsize too wide")
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  ####what if subsetInd is empty??
\end_layout

\begin_layout Plain Layout

  ParaTrials <- temp[, subsetInd]
\end_layout

\begin_layout Plain Layout

  TrialValues <- Pfun(b1=ParaTrials[1,], b2=ParaTrials[2,])
\end_layout

\begin_layout Plain Layout

  minInd <- which.min(trialValues)
\end_layout

\begin_layout Plain Layout

  PfunL <- TrialValues[minInd]
\end_layout

\begin_layout Plain Layout

  minParaOLD <- minPara <- ParaTrials[, minInd]
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  for (N in 1:25) {
\end_layout

\begin_layout Plain Layout

    if (sum(as.numeric(subsetInd)) < 15)
\end_layout

\begin_layout Plain Layout

      stepsize <- stepsize/3
\end_layout

\begin_layout Plain Layout

    if ( Pfun >= PfunLoid ) {
\end_layout

\begin_layout Plain Layout

      minPara <- minParaOLD
\end_layout

\begin_layout Plain Layout

      stepsize<-stepsize/3
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

    b1vec <- minPara[1] + stepsize[1] * (1:6 -3.5)
\end_layout

\begin_layout Plain Layout

    b2vec <- minPara[2] + stepsize[2] * (1:6 -3.5)
\end_layout

\begin_layout Plain Layout

    temp[1, ] <- rep(b1vec, each=6)
\end_layout

\begin_layout Plain Layout

    temp[2, ] <- rep(b1vec)
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    for (i in 1:6) for (j in 1:6 ) {
\end_layout

\begin_layout Plain Layout

      ind <- (i - 1) * 6 + j
\end_layout

\begin_layout Plain Layout

      mtemp <- LLRfn(para=c(b1vec[i], b2vec[j]), dataMat)
\end_layout

\begin_layout Plain Layout

      temp[3, ind] <- mtemp$"-2LLR"
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

    subsetInd <- (temp[3, ] <- target)
\end_layout

\begin_layout Plain Layout

    ParaTrials <- temp[, subsetInd]
\end_layout

\begin_layout Plain Layout

    TrialValues <- Pfun(b1=ParaTrials[1,], b2=ParaTrials[2,])
\end_layout

\begin_layout Plain Layout

    minInd <- which.min(TrialValues)
\end_layout

\begin_layout Plain Layout

    PfunLoid <- PfunL
\end_layout

\begin_layout Plain Layout

    PfunL <- TrialValues[minInd]
\end_layout

\begin_layout Plain Layout

    minParaOLD <- minPara
\end_layout

\begin_layout Plain Layout

    minPara <- ParaTrials[, minInd]
\end_layout

\begin_layout Plain Layout

    print(c(PfunL, minPara))
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  list(Lower=PfunL, minParameterNloglik = minPara)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Build R Package Using Rtools on Windows
\end_layout

\begin_layout Subsection
Prerequisite
\end_layout

\begin_layout Standard
In order to use Rtools to build R package, you have to make sure you have
 the following softwares already installed on your MS Windows computer:
\end_layout

\begin_layout Itemize
Latex (i.e.
 Miktex)
\end_layout

\begin_layout Itemize
R
\end_layout

\begin_layout Itemize
Rtools
\end_layout

\begin_layout Standard
Note that the version of R (newest 3.1.2) has to match the version of Rtools
 (newest Rtools32).
 You can check the existence of these softwares by typing in 
\begin_inset Quotes eld
\end_inset

latex
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

R
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

ls
\begin_inset Quotes erd
\end_inset

 in your DOS Prompt.
 And make sure that you have R and Rtools paths in your system environment.
\end_layout

\begin_layout Subsection
General Steps
\end_layout

\begin_layout Enumerate
Use package.skeleton() in R.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

First, you already have the function 
\begin_inset Quotes eld
\end_inset

Pfun
\begin_inset Quotes erd
\end_inset

 constructed in R.
 And you want to build up a package only containing this function.
 Then, to create a package called 
\begin_inset Quotes eld
\end_inset

STA695MZ
\begin_inset Quotes erd
\end_inset

 only containing 
\begin_inset Quotes eld
\end_inset

Pfun
\begin_inset Quotes erd
\end_inset

, use the R command.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Verbatim

package.skeleton(list="Pfun",name="Sta695MZ")
\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Editing the files
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

A package consists of a directory containing a file DESCRIPTION and usually
 has the subdirectories R, data and man.
 The package directory should be given the same name as the package.
 The package.skeleton command above will have created these files for you.
 You now need to edit them so they contain the right information.
\end_layout

\begin_deeper
\begin_layout Enumerate
DESCRIPTION file
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The DESCRIPTION file contains basic information about the package in the
 following format:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
Package: Sta695MZ
\end_layout

\begin_layout Plain Layout
Type: Package
\end_layout

\begin_layout Plain Layout
Title: Functions we used inn sta695 in Univ.
 of Kentucky
\end_layout

\begin_layout Plain Layout
Version: 0.1
\end_layout

\begin_layout Plain Layout
Date: 2015-02-26
\end_layout

\begin_layout Plain Layout
Author: Mai Zhou
\end_layout

\begin_layout Plain Layout
Maintainer: Mai Zhou <mai@ms.uky.edu>
\end_layout

\begin_layout Plain Layout
Description: Functions to construct likelihood ratio confidence intervals.
\end_layout

\begin_layout Plain Layout
License: GPL-2
\end_layout

\begin_layout Plain Layout
Packaged: 2015-03-03 03:01:39 UTC; mai
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Enumerate
Rd files
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The help files for each function and data set are given in R documentation
 (Rd) files in the man subdirectory.
 These are in a simple markup language closely resembling LaTeX, which can
 be processed into a variety of formats, including LaTeX, HTML and plain
 text.
 As an example, here is the file which documents the function Pfun in the
 Sta695MZ package.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\backslash
name{Pfun}
\end_layout

\begin_layout Plain Layout

\backslash
alias{Pfun}
\end_layout

\begin_layout Plain Layout
%- Also NEED an '
\backslash
alias' for EACH other topic documented here.
\end_layout

\begin_layout Plain Layout

\backslash
title{ Weibull survival probability at T=15.
 }
\end_layout

\begin_layout Plain Layout

\backslash
description{ A function that returns the survival probability at T=15, for
 the Weibull distribution.
 }
\end_layout

\begin_layout Plain Layout

\backslash
usage{ Pfun(b1, b2) } 
\backslash
arguments{ 
\backslash
item{b1}{ A positive number or vector.
 }
\end_layout

\begin_layout Plain Layout

\backslash
item{b2}{ A positive number.
 } }
\end_layout

\begin_layout Plain Layout

\backslash
details{ Returns a survival probability given the two parameters of the
 Weibull distribution.
 }
\end_layout

\begin_layout Plain Layout

\backslash
value{ The value returned is between 0 and 1.
 The survival probability at T=15 for the Weibull distribution, parameter
 b1 and b2.
 %%Here we use the definition of the Weibull as 
\backslash
code{ P(X > t) = exp( -(b1*t)^b2 ) }.
 }
\end_layout

\begin_layout Plain Layout

\backslash
references{ %% ~put references to the literature/web site here ~ }
\end_layout

\begin_layout Plain Layout

\backslash
author{ Mai Zhou }
\end_layout

\begin_layout Plain Layout

\backslash
note{ %% ~~further notes~~ }
\end_layout

\begin_layout Plain Layout
%% ~Make other sections like Warning with 
\backslash
section{Warning }{....} ~
\end_layout

\begin_layout Plain Layout

\backslash
seealso{ %% ~~objects to See Also as 
\backslash
code{
\backslash
link{help}}, ~~~ }
\end_layout

\begin_layout Plain Layout

\backslash
examples{ ##---- Should be DIRECTLY executable !! ---- Pfun(b1=0.1, b2=1.3)
 } 
\backslash
keyword{ distribution }
\end_layout

\begin_layout Plain Layout

\backslash
keyword{ survival }
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Checking the package
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

To check that the package satisfies the requirements for a CRAN package,
 in DOS Prompt, use
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Verbatim

Rcmd check STA695MZ
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\series bold
Notice:
\series default
 The checks are quite strict.
 A package will often work OK even if it doesn't pass these tests.
 But it is good practice to build packages that do satisfy these tests as
 it may save problems later.
\end_layout

\begin_layout Enumerate
Compiling the package for Windows
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

To compile the package into a zip file, go to a DOS prompt in the directory
 containing your package.
 (i.e., the directory 
\begin_inset Quotes eld
\end_inset

C:
\backslash
My Documents
\backslash

\begin_inset Quotes erd
\end_inset

 in the above example.
 Then type
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Verbatim

Rcmd INSTALL --build STA695MZ
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

This will compile all the necessary information and create a zip file which
 should be ready to load in R.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Enumerate
Building a package tar.gz file
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

To build a package as a tar.gz file, use
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Verbatim

Rcmd build STA695MZ
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

This creates a tar.gz file which can then be installed on a non-Windows computer.
 It can also be uploaded to CRAN provided it satisfies the above tests.
\end_layout

\begin_layout Enumerate
Installing a package from local zip files
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

In R, select Packages
\begin_inset Formula $\rightarrow$
\end_inset

Install packages from local zip files.
 Then select the zip file in your computer.
\end_layout

\begin_layout Enumerate
Including C code
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Whenever you want to build some packages that contains come C codes, you
 have to make sure that you have the source C code in the folder 
\begin_inset Quotes eld
\end_inset

src
\begin_inset Quotes erd
\end_inset

.
 In the following example, we try to build a 
\begin_inset Quotes eld
\end_inset

Cdemo
\begin_inset Quotes erd
\end_inset

 Package containing a function 
\begin_inset Quotes eld
\end_inset

addone
\begin_inset Quotes erd
\end_inset

 that will use the 
\begin_inset Quotes eld
\end_inset

Caddone
\begin_inset Quotes erd
\end_inset

 function in C.
 The 
\begin_inset Quotes eld
\end_inset

addone
\begin_inset Quotes erd
\end_inset

 function is:
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

addone <- function(x){
\end_layout

\begin_layout Plain Layout

temp <- .C("Caddone", as.double(x), as.integer(length(x)))
\end_layout

\begin_layout Plain Layout

return(temp[[1]])
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset

Then in the 
\begin_inset Quotes eld
\end_inset

src
\begin_inset Quotes erd
\end_inset

 folder under 
\begin_inset Quotes eld
\end_inset

Cdemo
\begin_inset Quotes erd
\end_inset

 folder, we add a file 
\begin_inset Quotes eld
\end_inset

Cadd.c
\begin_inset Quotes erd
\end_inset

 to provide the C code for the function 
\begin_inset Quotes eld
\end_inset

Caddone
\begin_inset Quotes erd
\end_inset

.
 The 
\begin_inset Quotes eld
\end_inset

Cadd.c
\begin_inset Quotes erd
\end_inset

 file is
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

void Caddone(double *x, int *length){
\end_layout

\begin_layout Plain Layout

  for(int i=0; i<*length; i++){ x[i] = x[i]+1;   }}
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset

Then you can build up the package and install it like what we did before.
\end_layout

\begin_layout Section
Big Data Analysis:
\end_layout

\begin_layout Standard
After the begging of the digital age about fifteen years ago, big data analysis
 became one of the most challenging problem in Statistics.
 Nowadays we can store a huge data in computers and keep it for a long time.
 We can also consider storing every single variable (or observation) no
 matter how important that variable is.
 An example of a big data is financial data ( oil prices, stocks, banking,
 dealers ...).
 This kind of data is always very huge and involve a lot of variables that
 affect everyday prices.
 An other example of big data is the bioinformatics data.
 Bioinformatics studies focus on analyzing and correlating genomic and proteomic
 information.
 As increasingly large amounts of genomic information, including both genomic
 sequences and expressed gene sequences, becomes available, statisticians
 face more difficulties to deal with this kind of data.
 Also customers behaviors (grocery stores, restaurants, services...) and internet
 users preferences (Google, yahoo, amazon, ...) are very huge data, and studying
 this data make advertising much more specific and targeting.On the other
 hand, new technologies allow us to read different types of data ( numeric,
 text, pictures, voice, and video).
 This makes analysis even more complicated.
 
\end_layout

\begin_layout Standard
So the statistical problem is how to build a model to explain and predict
 the future of studied phenomenon.
 And for sure, statisticians need to have the appropriate tools ( powerful
 computers with huge data storage, and efficient statistical softwares)
 to deal with this kind of data.
 In other words, if you have a statistical model with thousands of predictor
 variables, how to choose the most influential predictors to use in your
 model? Or how to reduce the dimension of your data without losing of any
 essential information?
\end_layout

\begin_layout Subsection
Betting strategies in gambling:
\end_layout

\begin_layout Standard
Betting strategies in gambling is one of the big data problems nowadays.
 For instance, building a model to better predict the best betting strategies
 on horse racing encounter dealing with hundreds of predictor variables.
 One of the common statistical methods to analyze this data is to build
 a logistic regression model or a probit model (in probit model we use normal
 cdf as a link function) to predict the future outcomes of gambling games.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
That is, if we have 12 horses in a each race, and consider that we have
 the outcomes of a certain two years of everyday's races of those 12 horses
 with all predictor variables that might significantly affect the race outcomes.T
hen by the the end of each race each horse has its own rank (running time)
 and we are going to treat this ranks (running times) as our response variable.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
One of the logistic regression models that used to study these data is the
 Conditional Logistic Regression Model or The Stratified Cox Model, which
 is a logistic regression model that maximizes the conditional likelihood.
 Let 
\begin_inset Formula $Y_{i},\,i=1,2,...,12$
\end_inset

 represent the rank (or the running times) of each horse after each race.
 And let's assume that we are interested in computing the probability of
 winning of each horse, i.e.
 
\begin_inset Formula $P(Y_{i}<min(Y_{i},Y_{2},...,Y_{12})),\,\forall\,i=1,2,...,12$
\end_inset

.
 Then our goal is to find the values of the parameters that maximize this
 probability.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
If we consider that 
\begin_inset Formula $Y_{i's}$
\end_inset

 are independent exponential random variables with rates 
\begin_inset Formula $\lambda_{i's}\,\forall\,i=1,2,...,12$
\end_inset

 then according to the conditional logistic model:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y_{i}<min(Y_{i},Y_{2},...,Y_{12}))=\frac{\lambda_{i}}{\sum_{k=1}^{12}\lambda_{k}},\,\forall\,i=1,2,...,12$
\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\lambda_{i}$
\end_inset

 is the strength of horse 
\begin_inset Formula $i$
\end_inset

 with 
\begin_inset Formula $\lambda_{i}=e^{-(\beta_{1}^{(i)}X_{1}+\beta_{2}^{(i)}X_{2}+\cdots+\beta_{p}^{(i)}X_{p})}$
\end_inset

, and 
\begin_inset Formula $p$
\end_inset

 is the number of predictors.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\beta_{j}^{(i)}:$
\end_inset

 The regression coefficient of predictor 
\begin_inset Formula $j$
\end_inset

 with 
\begin_inset Formula $j=1,2,...,p$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
So that we can make the probability of what we have observed in real life
 as high as possible by maximizing 
\begin_inset Formula $\frac{\lambda_{i}}{\sum_{k=1}^{12}\lambda_{k}}$
\end_inset

 .
 To do this we need to find the values of the unknown parameters 
\begin_inset Formula $\beta_{j's}$
\end_inset

 that maximize 
\begin_inset Formula $\lambda_{i}$
\end_inset

 using m.l.e or any other method of optimization to estimate the value of
 
\begin_inset Formula $\beta_{j's}$
\end_inset

.
\end_layout

\begin_layout Standard
However in reality we are not only interested in which horse win the race,
 but we want to figure out the probability of 12 horses crossed the finish
 line in a certain order ( say 4, 6, 1, 5, 7, 11, 2, 3, 9, 12, 10, and 8).
 Then the probability of this order is:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P(Y_{4} & <Y_{6}<Y_{1}<Y_{5}<Y_{7}<Y_{11}<Y_{2}<Y_{3}<Y_{9}<Y_{10}<Y_{8})\\
= & P(Y_{4}<min(Y_{1},Y_{2},...,Y_{12}))\times P(Y_{6}<min(Y_{1},Y_{2},Y_{3},Y_{5}...,Y_{12}))\times\cdots\times P(Y_{8}<min(Y_{8}))\\
= & \frac{\lambda_{4}}{\sum_{k=1}^{12}\lambda_{k}}\times\frac{\lambda_{6}}{\sum_{\begin{array}{c}
k=1\\
k\ne4
\end{array}}^{12}\lambda_{k}}\times\cdots\times\frac{\lambda_{8}}{\lambda_{8}}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

The first equality follows from the independence between 
\begin_inset Formula $Y_{i's}$
\end_inset

 and the memoryless property of exponential random variable.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Then the product of all probabilities of the observed orders of all races
 during the two years period of study is the likelihood.
 Then we can find the values of 
\begin_inset Formula $\beta_{j's}$
\end_inset

 that maximize this likelihood.
 
\end_layout

\begin_layout Standard
Note that we might consider only the first five or six horses in each race
 to calculate the probability for simplicity.
 Also note that 
\begin_inset Formula $Y_{i's}$
\end_inset

 not always exponentially distributed, they may follow Weibull distribution
 or normal distribution.
\end_layout

\begin_layout Section
High-dimensional Reduction
\end_layout

\begin_layout Subsection
Regression Model
\end_layout

\begin_layout Standard
A high-dimensional model is
\begin_inset Formula 
\begin{align*}
Y_{i} & =\alpha+\beta_{1}X_{1i}+\beta_{2}X_{2i}+\cdots+\beta_{10,000}X_{10,000i}+\epsilon_{i}\\
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $i=1,2,\cdots,n$
\end_inset

 are numbers of patients/tissues...
 
\begin_inset Formula $X_{i}$
\end_inset

's are normalized.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
In real world, the model is unrealistic because of the lack of degrees of
 freedom.
 In fact, most of the 
\begin_inset Formula $\beta$
\end_inset

's are irrelevant to the outcome, many of these 
\begin_inset Formula $\beta$
\end_inset

's are close to 0.
 So select the relevant 
\begin_inset Formula $\beta$
\end_inset

's becomes one of the important issue.
 
\end_layout

\begin_layout Standard
The problem raises now: even for only 9000 genes, picking 10 relavent genes
 has 
\begin_inset Formula $\left(\begin{array}{c}
9,000\\
10
\end{array}\right)$
\end_inset

 different ways.
 The combinations are huge.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The idea is to minimize the RSS(residual sum of squares), 
\begin_inset Formula $\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}}(\beta's))^{2}$
\end_inset

 subjects to, for example, 
\begin_inset Formula $\#\beta's\le10$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
However, the we still face the same problem of too many combinations.
 To overcome the difficulty, firstly, based on our present knowledge, we
 can keep some variables like gender, age, etc in the model and do the regressio
n, create a new 
\begin_inset Formula $Y_{i}'$
\end_inset

 and try to minimize RSS for the new 
\begin_inset Formula 
\begin{align*}
Y_{i}'= & Y_{i}-\hat{\alpha}-\hat{\beta_{1}}X_{1i}-\cdots-\hat{\beta_{p}}X_{pi}\\
= & \text{other variables that we don't know if we should keep in the model}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
LASSO Regression
\end_layout

\begin_layout Standard

\series bold
Now we introduce a new method --- LASSO Regression: Instead of restrict
 the number of 
\begin_inset Formula $\beta$
\end_inset

's, we now minimize RSS subjects to 
\begin_inset Formula $\sum_{i=1}^{n}|\beta_{i}|\le10$
\end_inset

.
 
\series default

\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
We can also subjects to, for example 
\begin_inset Formula $\sum_{i=1}^{n}|\beta_{i}|^{2}\le10$
\end_inset

 ( Ridge Regression ), or even 
\begin_inset Formula $\sum_{i=1}^{n}|\beta_{i}|^{\alpha}\le\text{some constant}.$
\end_inset

 But after some analysis, we know that only a convex minimize function subject
 to a convex function is solvable, which means it will give you a sparse
 solution, makes most ot 
\begin_inset Formula $\beta's=0.$
\end_inset

 And LASSO regression would be a good choice while Ridge regression is not
 sparse.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
(GRAPH NEEDED)
\end_layout

\begin_layout Subsection
Cross Validation
\end_layout

\begin_layout Standard
To solve LASSO regression, it's equivalent to solve: for a viven 
\begin_inset Formula $\lambda$
\end_inset

, 
\begin_inset Formula $\min\left[\sum(Y_{i}-\hat{Y_{i}})^{2}+\lambda\sum_{i=1}^{n}|\beta_{i}|\right]$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Here, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\lambda\sum_{i=1}^{n}|\beta_{i}|$
\end_inset

 is called LASSO penalty term and 
\begin_inset Formula $\lambda$
\end_inset

 is tuning parameter.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
The choice of 
\begin_inset Formula $\lambda$
\end_inset

 is quite important, where larger 
\begin_inset Formula $\lambda$
\end_inset

 could lead to few 
\begin_inset Formula $\beta's\ne0$
\end_inset

 and smaller 
\begin_inset Formula $\lambda$
\end_inset

 could lead to more 
\begin_inset Formula $\beta's\ne0.$
\end_inset

 The pick up of 
\begin_inset Formula $\lambda$
\end_inset

 is called 
\series bold
cross validation
\series default
.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}}(\beta's))^{2}=-\text{logLik}$
\end_inset

 if 
\begin_inset Formula $Y\sim\text{Normal}$
\end_inset

, it can be replaced by any convex function in 
\begin_inset Formula $\beta$
\end_inset

 
\begin_inset Formula $\Rightarrow$
\end_inset

min
\begin_inset Formula $\left\{ \text{convex}(\beta)+\lambda\sum_{j=1}^{p}|\beta_{j}|\right\} $
\end_inset

.
 It implies 
\begin_inset Formula $\text{convex}(\beta)=-\text{logLik}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<warning=FALSE,message=FALSE,results='hide'>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

library(penalized)
\end_layout

\begin_layout Plain Layout

data(nki70)
\end_layout

\begin_layout Plain Layout

pen <- penalized(Surv(time,event),penalized=nki70[,8:77],
\end_layout

\begin_layout Plain Layout

                 unpenalized = ~ER+Age+Diam+N+Grade,
\end_layout

\begin_layout Plain Layout

                 data=nki70, 
\end_layout

\begin_layout Plain Layout

                 lambda1=1,steps=5)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

pen
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

plotpath(pen)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Cox Model and Horse Racing Data
\end_layout

\begin_layout Standard
In this section, the horse racing data and related notes are required and
 has been posted on this website: http://www.ms.uky.edu/~mai/sta705.html.
 The data file is a comma separated values file (*.csv) which gets rid of
 extra EXCEL features.
 After reading the file into R, we could take a quick look at its dimention,
 variable names, and also the first 6 rows.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<warning=FALSE,message=FALSE>>=
\end_layout

\begin_layout Plain Layout

library(RCurl)
\end_layout

\begin_layout Plain Layout

x <- getURL("http://www.ms.uky.edu/~mai/sta705/ZhouM2.csv")
\end_layout

\begin_layout Plain Layout

ZhouM2 <- read.csv(text = x, header=TRUE)
\end_layout

\begin_layout Plain Layout

dim(ZhouM2)
\end_layout

\begin_layout Plain Layout

names(ZhouM2) 
\end_layout

\begin_layout Plain Layout

head(ZhouM2)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

To fit the data with a propriate model, let's start with Exponential Regression
 model:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Y_{i}\sim Exponential(\lambda_{i})
\]

\end_inset

where 
\begin_inset Formula $Y_{i}$
\end_inset

 is the survival time, 
\begin_inset Formula $\lambda_{i}>0$
\end_inset

 is the hazard, 
\begin_inset Formula $i=1,2,\cdots,n$
\end_inset

.
 A smaller 
\begin_inset Formula $\lambda$
\end_inset

 makes the subject live longer, and a larger 
\begin_inset Formula $\lambda$
\end_inset

 makes the subject live shorter.
 It is also true that 
\begin_inset Formula $P(Exp(\lambda)\le t)=1-e^{-\lambda t},$
\end_inset

 
\begin_inset Formula $\lambda_{i}=e^{\beta_{0}+\beta_{1}X_{1i}+\beta_{2}X_{2i}+\cdots+\beta_{p}X_{pi}}$
\end_inset

.
 Take the log of 
\begin_inset Formula $\lambda$
\end_inset

, we will get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
log\lambda_{i}=\beta_{0}+\beta_{1}X_{1i}+\beta_{2}X_{2i}+\cdots+\beta_{p}X_{pi}
\]

\end_inset

There is no random variable in this relation because there's no 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

.
 For the Exponential Regression model, proc lifetest in SAS and survreg
 in R can find its MLE for us.
 But we need 
\begin_inset Formula $Y_{i}'s$
\end_inset

, the survival time, to fit the model.
 Since we only have the ranks of horses instead of the running time, Exponential
 Regression seems not appropriate in this case.
 Cox model, however, is a perfect option.
 It's similar with Exponential Regression model, but it only uses ranks
 of 
\begin_inset Formula $Y_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
In Cox model, we do not want exponential distribution because of its memoryless
 property.
 So a monotone transformation (e.g.
 square, square root, log, etc.) of the 
\begin_inset Formula $Y_{i}$
\end_inset

 needs to be performed, and then we will have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g(Y_{i})\quad where\;Y_{i}\sim Exponential(\lambda_{i})
\]

\end_inset

After the transformation, it no longer has the memoryless property and loses
 magnitude, but we are able to keep the orders of 
\begin_inset Formula $Y_{i}$
\end_inset

.
 In terms of the horse racing data, a large 
\begin_inset Formula $\lambda$
\end_inset

 means a low rank which is good, and a small 
\begin_inset Formula $\lambda$
\end_inset

 indicates that the horse falls behind and has a high rank.
\end_layout

\begin_layout Standard
Another way to understand the Cox model is to look at it in the survival
 analysis point of view.
 Then we will have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{i}(t)=h_{0}(t)e^{\beta X_{i}}
\]

\end_inset

where 
\begin_inset Formula $h_{0}(t)$
\end_inset

 is the baseline, and the exponential component is a multiplication of the
 baseline.
\end_layout

\begin_layout Standard
For horse racing, it only makes sense if we compare the ranks within one
 race.
 So a stratified Cox model should be used this time.
 Here is the R code.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<warning=FALSE,message=FALSE>>=
\end_layout

\begin_layout Plain Layout

A <- ZhouM2[ ,3] == ZhouM2[1,3] # get all turf A data
\end_layout

\begin_layout Plain Layout

ZhouM2A <- ZhouM2[A, ]
\end_layout

\begin_layout Plain Layout

dim(ZhouM2A)
\end_layout

\begin_layout Plain Layout

names(ZhouM2A) #"Rank" is g(y_i)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

library(survival)
\end_layout

\begin_layout Plain Layout

## Cox proportional hazard regression model
\end_layout

\begin_layout Plain Layout

## want to determine rank, with the strata of date and race number
\end_layout

\begin_layout Plain Layout

coxph(Surv(Rank) ~ FinalOdds + WtCarried + JWinPer + 
\end_layout

\begin_layout Plain Layout

        JPlacePer + TWinPer + strata(Raceno + DateValue), data = ZhouM2A)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the output we want a high likelihodd ratio.
 And certainly one can create itneraction terms, or transformations of the
 terms to increase it.
 
\end_layout

\begin_layout Section
PGA and OGA
\end_layout

\begin_layout Standard
For a high dimentional model 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Y_{i}=\beta_{0}+\beta_{1}X_{1i}+\beta_{2}X_{2i}+\cdots+\beta_{p}X_{pi}+\varepsilon_{i}\qquad i=1,\,2,\,\cdots,\,n
\]

\end_inset

where 
\begin_inset Formula $p=1000,\,n=200$
\end_inset

 , we can use LASSO to do the analysis.
 There's a competitive method called PGA, or Step Forward procedure, can
 also be applied to estimate the parameters.
\end_layout

\begin_layout Enumerate
Find current residuals and current 
\begin_inset Formula $\hat{\beta}$
\end_inset

.
 To begin with, use 
\begin_inset Formula $Y-\bar{Y}$
\end_inset

 as the initial residuals and 
\begin_inset Formula $0$
\end_inset

 as the initial 
\begin_inset Formula $\hat{\beta_{i}}'s$
\end_inset


\end_layout

\begin_layout Enumerate
Maximize 
\begin_inset Formula $\left[corr(residual,\,X_{j})\right]^{2}$
\end_inset

and record the 
\begin_inset Formula $X_{j}$
\end_inset

 that maximizes it.
 Do a simple regression on residuals and 
\begin_inset Formula $X_{j}$
\end_inset

, then we will find out its parameter 
\begin_inset Formula $\beta_{j}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Update 
\begin_inset Formula $\hat{\beta_{i}}'s$
\end_inset

.
 
\begin_inset Formula $\hat{\beta}_{new}[j]=\hat{\beta}[j]+\lambda\beta_{j}$
\end_inset

 where 
\begin_inset Formula $0<\lambda<1$
\end_inset

 and it's a tuning parameter.
\end_layout

\begin_layout Enumerate
Compute residuals again and repeat the process.
\end_layout

\begin_layout Standard
One disadvantage of PGA is that we don't know when it's going to stop.
 Thus, a PGA+OGA procudure is recommanded.
 OGA stands for Orthogonal Greedy Algorithm.
 It usually produces sparse solutions (a lot of 
\begin_inset Formula $0$
\end_inset

 in 
\begin_inset Formula $\hat{\beta_{i}}'s$
\end_inset

 ), which are indeed what we want.
 
\end_layout

\begin_layout Section
Likelihood problems and Partial Likelihood
\end_layout

\begin_layout Subsection
Likelihood Problems
\end_layout

\begin_layout Subsubsection
Neymann-Scott Example
\end_layout

\begin_layout Standard
This is an example where the likelihood equation does not work in producing
 a consistent estimator.
 Suppose we have the following sample:
\begin_inset Formula 
\[
x_{11},x_{12},\ldots,x_{16}\sim N(\mu_{1},\sigma^{2})
\]

\end_inset

 The fact that we have a sample size of 6 is arbitrary.
 With this sample the MLE's for 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 are
\begin_inset Formula 
\[
\hat{\mu}_{1}=\bar{x}_{1}\ and\ \hat{\sigma^{2}}=\frac{1}{6}\sum_{i=1}^{6}(x_{1i}-\bar{x}_{1})^{2}
\]

\end_inset

 these are just the usual MLE's for a normal distribution.
 Now suppose we add the following to our sample 
\begin_inset Formula 
\[
x_{21},x_{22},\ldots,x_{26}\sim N(\mu_{2},\sigma^{2})
\]

\end_inset

 Our MLE for 
\begin_inset Formula $\mu_{1}$
\end_inset

 stays the same and we have a similar MLE for 
\begin_inset Formula $\mu_{2}$
\end_inset

, but our 
\begin_inset Formula $\hat{\sigma^{2}}$
\end_inset

 will change.
 
\begin_inset Formula 
\[
\hat{\mu}_{2}=\bar{x}_{2}\ and\ \hat{\sigma^{2}}=\frac{\sum_{i=1}^{6}(x_{1i}-\bar{x}_{1})^{2}+\sum_{i=1}^{6}(x_{2i}-\bar{x}_{2})^{2}}{12}
\]

\end_inset

 Following in this fashion, if we have a total of n samples with this similar
 distribution
\begin_inset Formula 
\[
x_{j1},x_{j2},\ldots,x_{j6}\sim N(\mu_{j},\sigma^{2})
\]

\end_inset

 then we will have the following MLEs
\begin_inset Formula 
\[
\hat{\mu}_{j}=\bar{x}_{j}\ for\ i=1,2,\ldots,n
\]

\end_inset


\begin_inset Formula 
\[
\hat{\sigma^{2}}=\frac{1}{6n}\sum_{j=1}^{n}\sum_{i=1}^{6}(x_{ji}-\bar{x}_{j})^{2}
\]

\end_inset

 The problem with this estimator for 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is that it is not consistent.
 We know that 
\begin_inset Formula $\frac{\sum_{i=1}^{6}(x_{ji}-\bar{x}_{j})^{2}}{\sigma^{2}}=y$
\end_inset

 is distributed as a 
\begin_inset Formula $\chi^{2}$
\end_inset

 with 5 degrees of freedom.
 Thus we can say 
\begin_inset Formula $E(\hat{\sigma^{2})}=\frac{n\sigma^{2}y}{6n}$
\end_inset

.
 By the law of large numbers we know that as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

, 
\begin_inset Formula $E(\hat{\sigma^{2}})\rightarrow\frac{5\sigma^{2}}{6}\neq\sigma^{2}$
\end_inset

.
 Therefore 
\begin_inset Formula $\hat{\sigma^{2}}$
\end_inset

 is not a consistent estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 And this is a problem.
 The reason that the MLE does not work in this example is because of the
 existence of infinitely many nuisance parameters, 
\begin_inset Formula $\mu_{j}$
\end_inset

's.
 All of the degrees of freedom are used up in estimating these infinitely
 many parameters that we are not interested in.
\end_layout

\begin_layout Subsubsection
Another Problem with the Likelihood
\end_layout

\begin_layout Standard
Let us suppose that we have the following sample:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{1},x_{2},\ldots,x_{n}\sim f(\cdot)
\]

\end_inset

 In this case our parameter of interest is the whole function 
\begin_inset Formula $f(\cdot)$
\end_inset

.
 This produces a problem, because 
\begin_inset Formula $max_{f(\cdot)}\{\prod_{i=1}^{n}f(x_{i})=\infty$
\end_inset

.
 You can always find a function that is higher than your previously found
 function.
\end_layout

\begin_layout Standard
The likelihood, MLE, score, and information can have problems when either
 the number of nuisance parameters grows as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 or when the parameter itself is a function.
\end_layout

\begin_layout Subsection
Cox and the Partial Likelihood
\end_layout

\begin_layout Subsubsection
Introduction of the Partial Likelihood
\end_layout

\begin_layout Standard
When you have a lot of nuisance parameters and the normal likelihood equation
 does not work, you can use Cox's Partial Likelihood.
 The example he gives has each observation consisting of the elements 
\begin_inset Formula $(T_{i},x_{i},\delta_{i})\ for\ i=1,2,\ldots,n$
\end_inset

.
 Where 
\begin_inset Formula $T_{i}$
\end_inset

 is the observed survival time, 
\begin_inset Formula $x_{i}$
\end_inset

 is the covariate, and 
\begin_inset Formula $\delta_{i}$
\end_inset

 is the censor status, all related to the 
\begin_inset Formula $i^{th}$
\end_inset

 patient.
 In this example the partial likelihood, denoted 
\begin_inset Formula $PL$
\end_inset

 is 
\begin_inset Formula 
\[
PL=\prod_{i=1}^{n}\left(\frac{e^{\beta x_{i}}}{\sum_{j=1}^{n}e^{\beta x_{i}}I[T_{j}\geq T_{i}]}\right)^{\delta_{i}}
\]

\end_inset

 This is in a sense related to the probability of the rank of the 
\begin_inset Formula $T_{i}$
\end_inset

's, not the 
\begin_inset Formula $T_{i}$
\end_inset

's themselves.
\end_layout

\begin_layout Standard
We have a model assumption that 
\begin_inset Formula $T_{i}$
\end_inset

 has a hazard function given by 
\begin_inset Formula 
\[
h_{i}(t)=h_{0}(t)e^{\beta x_{i}}
\]

\end_inset

 where 
\begin_inset Formula $h_{i}(t)$
\end_inset

 represents the hazard function of the 
\begin_inset Formula $i^{th}$
\end_inset

 patient and 
\begin_inset Formula $h_{0}(t)$
\end_inset

 is the baseline hazard function.
 Here 
\begin_inset Formula $\beta$
\end_inset

 is the parameter of interest and 
\begin_inset Formula $h_{0}(t)$
\end_inset

 is a nuisance parameter of infinite dimension.
\end_layout

\begin_layout Standard
The question we need to ask ourselves is if this baseline hazard function
 is going to destroy the use of the likelihood.
\end_layout

\begin_layout Subsubsection
Revisiting the Hazard Function
\end_layout

\begin_layout Standard
The hazard function is of the form 
\begin_inset Formula 
\[
h(t)=\frac{f(t)}{1-F(t)}
\]

\end_inset

 The cumulative hazard function, sometimes denoted 
\begin_inset Formula $H(t)$
\end_inset

 is simply
\begin_inset Formula 
\[
\Lambda(t)=\int h(s)ds
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Steps to Deriving the Partial Likelihood Equation
\end_layout

\begin_layout Standard
The steps to deriving the Partial Likelihood of the Cox Model are quite
 simple.
\end_layout

\begin_layout Enumerate
Write down the likelihood for the Cox Model.
 Make sure to put it in terms of the baseline hazard function, 
\begin_inset Formula $h_{0}(\cdot)$
\end_inset

, and 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Enumerate
Profile out the baseline hazard function, 
\begin_inset Formula $h_{0}(\cdot)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Produce the Partial Likelihood equation.
\end_layout

\begin_layout Standard
Some things to point out before we go through this process is that it is
 not clear if we are able to profile out the 
\begin_inset Formula $\mu_{i}$
\end_inset

's in the Neymann-Scott example.
 Also the Partial Likelihood may not work in some situations.
 We will work through the above steps for the Cox Model.
\end_layout

\begin_layout Enumerate
The likelihood of the Cox Model is as follows:
\begin_inset Formula 
\begin{align*}
Lik & =\prod_{i=1}^{n}f(x_{i})\\
 & =\prod_{i=1}^{n}\frac{f(x_{i})}{1-F(x_{i})}\left[1-F(x_{i})\right]\\
 & =\prod_{i=1}^{n}h(x_{i})e^{-\Lambda(x_{i})}
\end{align*}

\end_inset

 We can move from lines 2 to 3, because we know that 
\begin_inset Formula $e^{-\Lambda(x_{i})}=1-F(x_{i})$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
For some reason I do not know how we got from this step to the the following
 one
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{align*}
Lik & =\prod_{i=1}^{n}\left\{ f_{i}(t_{i})dt*I\left[\delta_{i}=1\right]+\left[1-F_{i}(t_{i})\right]I\left[\delta_{i}=0\right]\right\} \\
 & =\prod_{i=1}^{n}\left\{ \frac{f_{i}(t_{i})dt*e^{-\Lambda_{i}(t_{i})}}{1-F_{i}(t_{i})}I\left[\delta_{i}=1\right]+e^{-\Lambda_{i}(t_{i})}I\left[\delta_{i}=0\right]\right\} \\
 & =\prod_{i=1}^{n}\left[h_{i}(t_{i})dt\right]^{\delta_{i}}e^{-\int_{0}^{t_{i}}h_{i}(s)ds}
\end{align*}

\end_inset

 By using the Cox model assumption mentioned earlier, we can manipulate
 the likelihood equation even more.
\begin_inset Formula 
\[
Lik=\prod_{i=1}^{n}\left[h_{0}(t_{i})e^{\beta x_{i}}dt\right]^{\delta_{i}}e^{-\int_{0}^{t_{i}}h_{0}(s)e^{\beta x_{i}}ds}
\]

\end_inset

 By taking the log of each side of the equation and letting 
\begin_inset Formula $w_{i}=h_{0}(t_{i})dt$
\end_inset

 for 
\begin_inset Formula $i=1,2,\ldots,n$
\end_inset

 
\begin_inset Formula 
\[
logLik(\beta,\vec{w})=\sum_{i=1}^{n}\left\{ \delta_{i}\left[\beta x_{i}+log(w_{i})\right]-e^{\beta x_{i}}\int_{0}^{t_{i}}w_{s}\right\} 
\]

\end_inset

 We know that 
\begin_inset Formula $w_{i}$
\end_inset

must be discrete.
 If it was continuous, there would be no maximum.
 Therefore we can change the integral to a summation.
 
\begin_inset Formula 
\[
logLik(\beta,\vec{w})=\sum_{i=1}^{n}\left\{ \delta_{i}\left[\beta x_{i}+log(w_{i})\right]-e^{\beta x_{i}}\sum_{s=0}^{t_{i}}w_{s}\right\} 
\]

\end_inset

 In this example 
\begin_inset Formula $w_{i}$
\end_inset

 is considered to be a nuisance parameter because it continues to grow as
 n increases.
 In this case the logLik still works, unlike the Neymann-Scott example.
 Here is a reminder of what the logPL looks like, and that it does not involve
 
\begin_inset Formula $\vec{w}$
\end_inset

.
\begin_inset Formula 
\[
logPL=\sum_{i=1}^{n}\delta_{i}\left[\beta x_{i}-log\left(\sum_{j=1}^{n}e^{\beta x_{i}}I\left[T_{j}\geq T_{i}\right]\right)\right]
\]

\end_inset


\end_layout

\begin_layout Enumerate
Profiling out the 
\begin_inset Formula $w_{i}$
\end_inset

's is based on a theorem.
 For any given and fixed 
\begin_inset Formula $\beta=\beta^{*}$
\end_inset

, we can find the solution to 
\begin_inset Formula $max_{\vec{w}}\left\{ logLik(\beta^{*},\vec{w})\right\} $
\end_inset

.
 The solution is
\begin_inset Formula 
\[
w_{i}(\beta^{*})=\frac{\delta_{i}}{\sum_{j=1}^{n}e^{\beta x_{i}}I[T_{j}\geq T_{i}]}
\]

\end_inset

 The values of these 
\begin_inset Formula $w_{i}(\beta^{*})$
\end_inset

's will be the MLEs if the 
\begin_inset Formula $\beta^{*}$
\end_inset

 is the MLE of 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Enumerate
Now we can see the relationship between logLik and logPL when the solution
 
\begin_inset Formula $w_{i}(\beta^{*})$
\end_inset

 is plugged in.
\begin_inset Formula 
\[
logLik(\beta^{*},w_{i}(\beta^{*}))=logPL(\beta^{*})-\sum_{i=1}^{n}\delta_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
In this example we can use the partial likelihood if we would like to do
 any inference such as confidence intervals and hypothesis testing just
 involving the 
\begin_inset Formula $\beta$
\end_inset

's as this equation is much easier to work with.
 However if we would like to perform any inferences involving the 
\begin_inset Formula $w_{i}$
\end_inset

's, then we would have to use the original likelihood equation.
\end_layout

\begin_layout Subsubsection
Using R to Compute Empirical and Partial Likelihoods
\end_layout

\begin_layout Standard
The package ELYP, written by Dr.
 Zhou, contains a function CoxEL.
 This function computes the empirical likelihood as well as the partial
 likelihood for the purpose of testing the parameter 
\begin_inset Formula $\beta$
\end_inset

 and the baseline hazard feature, which Dr.
 Zhou defines to be lam and fun.
\end_layout

\begin_layout Standard
This function has 6 inputs: y, d, Z, beta, lam, and fun.
 In this case, y is equivalent to the 
\begin_inset Formula $t_{i}$
\end_inset

 used above, d is equivalent to 
\begin_inset Formula $\delta_{i}$
\end_inset

, Z is equivalent to 
\begin_inset Formula $x_{i}$
\end_inset

, and beta is equivalent to 
\begin_inset Formula $\beta^{*}$
\end_inset

.
 The new inputs that we haven's discussed yet are lam, which is a scalar
 used in the constraint of baseline, and fun, a function which together
 with lam defines the feature of the baseline hazard.
 When lam=0 there is no restriction on the 
\begin_inset Formula $w_{i}$
\end_inset

's which means that they will follow the formula 
\begin_inset Formula $w_{i}(\beta^{*})=\frac{\delta_{i}}{\sum_{j=1}^{n}e^{\beta x_{i}}I[T_{j}\geq T_{i}]}$
\end_inset

.
 Here is an example where y, d, and Z are defined below.
 We will set beta equal to 1, just to run the function, and lam equal to
 0.
 This means that it does not matter what function we put in, since it will
 not affect the 
\begin_inset Formula $w_{i}$
\end_inset

's at all.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

library("ELYP")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

y <- c(3, 5.3, 6.4, 9.1, 14.1, 15.4, 18.1, 15.3, 14, 5.8, 7.3, 14.4) 
\end_layout

\begin_layout Plain Layout

x <- c(1, 1.5, 2, 3, 4, 5, 6, 5, 4, 1, 2, 4.5) 
\end_layout

\begin_layout Plain Layout

d <- c(1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

CoxEL(y=y, d=d, Z=x, beta=1, lam=0, fun=function(t){t}) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
A brief explanation of the output of this function.
\end_layout

\begin_layout Itemize
d - This is a list of the inputted d values, sorted by the y values.
\end_layout

\begin_layout Itemize
Hazw - These are your 
\begin_inset Formula $w_{i}$
\end_inset

's.
 Remember that 
\begin_inset Formula $w_{i}=h_{0}(t_{i})dt$
\end_inset

 where 
\begin_inset Formula $h_{0}(t_{i})$
\end_inset

 is your baseline hazard function.
\end_layout

\begin_layout Itemize
mu - This is equal to 
\begin_inset Formula $\int f(t)h_{0}(t)dt$
\end_inset

.
 This is the value that is pulled either up or down by your lam input, where
 f(t) is the fun input.
\end_layout

\begin_layout Itemize
logPlik - This is the value of log partial likelihood.
\end_layout

\begin_layout Itemize
logEmpLik - This is the value of the log empirical likelihood.
\end_layout

\begin_layout Standard
You will notice that 
\begin_inset Formula $logEmpLik=logPlik-\sum_{i=1}^{12}d$
\end_inset

, which matches our derivation of the partial likelihood.
\end_layout

\begin_layout Standard
Since the beta input is not the MLE of 
\begin_inset Formula $\beta$
\end_inset

, the Hazw output are not the MLEs of the 
\begin_inset Formula $w_{i}$
\end_inset

's.
 This is how to find the MLE of 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

 
\end_layout

\begin_layout Plain Layout

library("survival")
\end_layout

\begin_layout Plain Layout

beta<-coef(coxph(Surv(y,d)~x))
\end_layout

\begin_layout Plain Layout

beta
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

This gives up the true MLE of 
\begin_inset Formula $\beta$
\end_inset

.
 This is what happens when we input this value of beta into the CoxEL function.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

CoxEL(y=y, d=d, Z=x, beta=beta, lam=0, fun=function(t){t})
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Standard
You will see that the outputs changes, with the exception of the d values.
 These Hazw's are now the MLEs of the 
\begin_inset Formula $w_{i}$
\end_inset

's.
 It is also important to note that this function retrieves the same value
 for the log partial log likelihood as the unconditional likelihood given
 by the coxph function.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

coxph(Surv(y,d)~x)$loglik
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Cox Partial Likelihood is a a good way to make inferences on the non-nuisanc
e parameters.
 However if you would like to find the MLEs of the nuisance parameters,
 CoxEL is a good way to find those estimates.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Subsubsection
Construct confidence interval for 
\begin_inset Formula $(\beta,\mu)$
\end_inset

 from inverting log likelihood
\end_layout

\begin_layout Standard
First, let us look at null the hypothesis below:
\begin_inset Formula 
\[
H_{0}:\begin{array}{c}
\mbox{ 300 days survival probability for a patient with covariate }Z\mbox{ =}c\\
e^{-\varLambda_{0}(300)e^{\beta Z}}=c
\end{array}.
\]

\end_inset

Note that even if we have two unkown things, 
\begin_inset Formula $\varLambda_{0}(300)$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 in the hypothesis, the number of parameter is only equal to one.
 And we could build up a confidence interval for the term 
\begin_inset Formula $e^{-\varLambda_{0}(300)e^{\beta Z}}$
\end_inset

 by inverting Likelihood Ratio Test (LRT).
\begin_inset Newline newline
\end_inset

Furthermore, we could decompose 
\begin_inset Formula $H_{0}$
\end_inset

 into many 
\begin_inset Formula $H_{00}'s$
\end_inset

, where
\begin_inset Formula 
\[
H_{00}:\begin{array}{c}
\mu=\varLambda_{0}(300)=\mu^{*}\\
\beta=\beta^{*}
\end{array}
\]

\end_inset

But here we have two parameters.
 And we could do the same thing, that is to invert LRT to get a joint confidence
 set (a region, maybe an interval) for 
\begin_inset Formula $(\beta,\mu)$
\end_inset

.
 Once we have the confidence set (a region), then the confidence interval
 for 
\begin_inset Formula $e^{-\varLambda_{0}(300)e^{\beta Z}}$
\end_inset

 is just the range that 
\begin_inset Formula $e^{-\varLambda_{0}(300)e^{\beta Z}}$
\end_inset

 can take on that region.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Let us focus on one specific 
\begin_inset Formula $H_{00}$
\end_inset


\begin_inset Formula 
\[
H_{00}:\begin{array}{c}
\mu=\varLambda_{0}(300)=0.2\\
\beta=0.5
\end{array}
\]

\end_inset

For the dataset 
\begin_inset Quotes eld
\end_inset

smallcell
\begin_inset Quotes erd
\end_inset

 in R, we can try to compute the true likelihood of survival for the above
 
\begin_inset Formula $H_{00}$
\end_inset

 using CoxEL function in 
\begin_inset Quotes eld
\end_inset

ELYP
\begin_inset Quotes erd
\end_inset

 package.
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

library("ELYP")
\end_layout

\begin_layout Plain Layout

data(smallcell)
\end_layout

\begin_layout Plain Layout

myy <- smallcell$survival
\end_layout

\begin_layout Plain Layout

myd <- smallcell$indicator
\end_layout

\begin_layout Plain Layout

myZ <- smallcell$arm
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

But we need to define 
\begin_inset Formula $f(x)$
\end_inset

 such that 
\begin_inset Formula $\int f(t)d\varLambda_{0}(t)=\varLambda_{0}(300)$
\end_inset

.
 And we could easily acheive this by defining 
\begin_inset Formula $f(t)=I[t\leq300]$
\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

myfun <- function(t){as.numeric(t<=300)}
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Now let's compute the likelihood
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

CoxEL(y=myy,d=myd,Z=myZ,beta=0.5,lam=0,fun=myfun)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Notice that, log likelihood is -500.907, but 
\begin_inset Formula $\mu$
\end_inset

 is not equal to 0.2, which is the value of 
\begin_inset Formula $\varLambda_{0}(300)$
\end_inset

 in 
\begin_inset Formula $H_{00}$
\end_inset

.
 Hence, we need to find a 
\begin_inset Formula $lam$
\end_inset

 values to make 
\begin_inset Formula $\mu=\varLambda_{0}(300)=0.2$
\end_inset

.
 After several trials, we find out that with 
\begin_inset Formula $lam=-15.95$
\end_inset

, we could have 
\begin_inset Formula $\mu$
\end_inset

 approximately 0.2, as shown below.
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

CoxEL(y=myy,d=myd,Z=myZ,beta=0.5,lam=-15.95,fun=myfun)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Hence the 
\begin_inset Formula $logLik_{0}=logLik(\beta=0.5,\mu=0.2)=-501.0959$
\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Recall the process of inverting LRT to get confidence interval for the parameter
s.
 We reject 
\begin_inset Formula $H_{00}$
\end_inset

 if 
\begin_inset Formula $\frac{Lik_{0}}{Lik_{A}}$
\end_inset

 is smaller than some number.
 That is, 
\begin_inset Formula $-2[logLik_{0}-logLik_{A}]<\mbox{some number}.$
\end_inset

 And since 
\begin_inset Formula $logLik_{A}=logLik(MLE)$
\end_inset

, all we need to do is to plug in 
\begin_inset Formula $\hat{\beta}_{Cox}$
\end_inset

 and set 
\begin_inset Formula $lam=0$
\end_inset

, as shown below.
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

library(survival)
\end_layout

\begin_layout Plain Layout

coxph(Surv(myy,myd)~myZ)$coef
\end_layout

\begin_layout Plain Layout

CoxEL(y=myy,d=myd,Z=myZ,beta=0.5337653,lam=0,fun=myfun)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Then we have 
\begin_inset Formula $logLik_{A}=-500.8937$
\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Then, 
\begin_inset Formula $-2[logLik_{0}-logLik_{A}]=2[logLik_{A}-logLik_{0}]=2[-500.8937+501.0959]=0.4044$
\end_inset

.
 And we've learned the that approximate distribution of 
\begin_inset Formula $-2[logLik_{0}-logLik_{A}]$
\end_inset

 is 
\begin_inset Formula $\chi^{2}$
\end_inset

 with 
\begin_inset Formula $df=2$
\end_inset

.
 We can calculate the p-value and decide whether to reject based on some
 confidence.
 If reject, we will include this 
\begin_inset Formula $(\beta^{*},\mu^{*})$
\end_inset

 in the confidence set.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

After we have all the possible 
\begin_inset Formula $\beta^{*}$
\end_inset

 and 
\begin_inset Formula $\mu^{*}$
\end_inset

, we might have a region.
 Then we need to find the range of the values that 
\begin_inset Formula $e^{-\varLambda_{0}(300)e^{\beta Z}}$
\end_inset

 can take on that region.
 And that will be our confidence for 
\begin_inset Formula $e^{-\varLambda_{0}(300)e^{\beta Z}}$
\end_inset

.
\end_layout

\end_body
\end_document
